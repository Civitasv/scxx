# 词法分析

我们今天实现词法分析。

不过，首先我们聊聊解释器是如何执行程序的。

第一，执行词法分析，常称为 lexer analysis，词法分析的目的是获取语言中的原子，或者叫最小组成部分。得到的结果往往称为 Tokens。
第二，对得到的 Tokens 进行语法层面上的解析，常称为 parser，目的是得到 AST，也就是抽象语法树。
第三，解析得到的 AST 并执行。

当然，这是简化后的流程，现代解释器还可能包括代码分析模块、代码优化模块等，但我们暂时不考虑。

## 语言设计

编写词法分析器前，我们需要设计我们的语言。

顶层是 Expr。

    Expr = (List, Atom)

Atom 包括 Symbol 和 Number。

其他的都称为 List，包括 Special 和 Call。

这里为什么将 List 区分为 Special 和 Call 呢。

这是因为 Scheme 执行代码往往分为两步：

第一，计算所有参数值；
第二，将计算得到的参数传入函数。

而像 if，define，quotation，lambda 却不符合这种计算流程，因此我们将它们称为 Special form，这个定义来源于 SICP。

Call 的形式是：(Func expr1 expr2 ...)，其中 Func 可以分为 Procedure 和 Primitive，Primitive 表示我们原始提供的函数，Procedure 表示用于定义的方法。

## 词法分析

那么，我们现在实现词法分析模块。词法分析的目的是获取语言中的原子，或者叫最小组成部分。得到的结果往往称为 Tokens。

对于这个简化了的 Scheme 语言，只存在左括号，右括号，字符串以及数字。因此，词法分析相当容易。

```cpp
#pragma once

#include <algorithm>
#include <iostream>
#include <string>
#include <unordered_map>
#include <vector>

#include "cv_string.h"
#include "type.h"

namespace scxx {

struct Token {
  enum Type { LEFT_PAREN, RIGHT_PAREN, SYMBOL, NUMBER };
  union Value {
    Number* number;
    Symbol* symbol;
  };

  Token(const Token& token) {
    switch (token.type) {
      case NUMBER:
        if (token.value.number) {
          type = NUMBER;
          value.number = new Number(*token.value.number);
        }
        break;
      case SYMBOL:
        if (token.value.symbol) {
          type = SYMBOL;
          value.symbol = new Symbol(*token.value.symbol);
        }
        break;
      default:
        type = token.type;
        break;
    }
  }

  Token(Type type) : type(type) {}
  Token(const Symbol& symbol) : type(SYMBOL) {
    value.symbol = new Symbol(symbol);
  }
  Token(const Number& number) : type(NUMBER) {
    value.number = new Number(number);
  }

  ~Token() {
    if (type == SYMBOL) {
      if (value.symbol) {
        delete value.symbol;
        value.symbol = nullptr;
      }
    } else if (type == NUMBER) {
      if (value.number) {
        delete value.number;
        value.number = nullptr;
      }
    }
  }

  Type type;
  Value value;

  friend std::ostream& operator<<(std::ostream& os, const Token& token) {
    switch (token.type) {
      case Token::LEFT_PAREN:
        os << "(";
        break;
      case Token::RIGHT_PAREN:
        os << ")";
        break;
      case Token::SYMBOL:
        os << "SYMBOL: " << *token.value.symbol;
        break;
      case Token::NUMBER:
        os << "NUMBER: " << *token.value.number;
        break;
      default:
        break;
    }
    return os;
  }
};

class Lexer {
 public:
  Lexer() {}

  /// @brief 进行词法分析，获取所有 tokens.
  /// @return 所有 tokens.
  std::vector<Token> Tokenize(std::string& source) {
    // 1. split the source by whitespace
    cv_string::Replace(source, "(", " ( ");
    cv_string::Replace(source, ")", " ) ");
    cv_string::Replace(source, "'", " ' ");
    std::vector<std::string> ss = cv_string::Split(source, " ");

    bool quote = false;
    std::vector<Token> tokens{};

    std::for_each(ss.begin(), ss.end(),
                  [&tokens, &quote](const std::string& s) {
                    if (s == "(") {
                      tokens.push_back(Token::LEFT_PAREN);
                    } else if (s == ")") {
                      if (quote) {
                        tokens.push_back(Token::RIGHT_PAREN);
                        quote = false;
                      }
                      tokens.push_back(Token::RIGHT_PAREN);
                    } else if (s == "'") {
                      quote = true;
                      tokens.push_back(Token::LEFT_PAREN);
                      tokens.push_back(Symbol("quote"));
                    } else if (cv_string::IsNumber(s)) {
                      tokens.push_back(std::stod(s));
                    } else {
                      tokens.push_back(s);
                    }
                  });
    return tokens;
  }
};
}  // namespace scxx
```

### cv_string

cv_string 存放一些与字符串相关的工具函数。

```cpp
/**
 * This is a single-header library. Followed by:
 * https://github.com/nothings/stb/blob/master/docs/stb_howto.txt To use it:
 *
 * To use this library:
 * 1. #define CV_STRING_IMPLENTATION
 * 2. #include "cv_string.h"
 */

#ifndef CVI_INCLUDE_CV_STRING_H
#define CVI_INCLUDE_CV_STRING_H

#include <cctype>
#include <regex>
#include <string>
#include <vector>

namespace cv_string {
void Replace(std::string& source, const std::string& from,
             const std::string& to);

std::vector<std::string> Split(const std::string& source,
                               const std::string& delimiter);

bool IsNumber(const std::string& str);
}  // namespace cv_string
#endif

#ifdef CV_STRING_IMPLENTATION
namespace cv_string {
const std::regex NUMBER_REGEX("^(-?)(0|([1-9][0-9]*))(\\.[0-9]+)?$");

// From https://stackoverflow.com/a/24315631/11112422
void Replace(std::string& source, const std::string& from,
             const std::string& to) {
  size_t start_pos = 0;

  while ((start_pos = source.find(from, start_pos)) != std::string::npos) {
    source.replace(start_pos, from.length(), to);
    start_pos += to.length();
  }
}

std::vector<std::string> Split(const std::string& source,
                               const std::string& delimiter) {
  std::vector<std::string> ss;
  size_t pos = 0, last = 0;
  std::string token;
  while ((pos = source.find(delimiter, last)) != std::string::npos) {
    token = source.substr(last, pos - last);
    if (token.size() > 0) ss.push_back(std::move(token));
    last = pos + delimiter.length();
  }

  if (last < source.size()) ss.push_back(source.substr(last));

  return ss;
}

bool IsNumber(const std::string& str) {
  std::smatch match;

  return std::regex_match(str, match, NUMBER_REGEX);
}

}  // namespace cv_string
#endif
```

### 测试

这里我采用 Approval test。

```cpp
TEST(SCXX, LEX_1) {
  std::string source = "(+ (+ 1 2) 2)";
  Lexer lexer;
  auto tokens = lexer.Tokenize(source);
  // auto converter = [](auto s, auto& os) { os << s << " => " << s[0]; };
  Approvals::verifyAll("TOKENS", tokens);
}
```

### run.cc

```cpp
void repl() {
  using namespace scxx;
  Environment* environment = StandardEnv();
  Lexer lexer;
  Parser parser;
  // read, eval, print, loop
  while (true) {
    print("1 ]=> ");
    std::string input = read();
    std::vector<Token> tokens = lexer.Tokenize(input);
    std::cout << ";Value: ";
    for(auto& item: tokens){
        std::cout << item << " ";
    }
    print("\n");
  }
  delete environment;
}
```
